{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechWhizGenius/Teja_INFO5731_Fall2024/blob/main/TERM_PROJECT/Scraping_PubMed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests xmltodict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ok6KAr0X2K2",
        "outputId": "cd003d9d-77ef-41dd-d3db-d22afb4f38d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (0.14.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xmltodict\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL for PubMed's E-Utilities API\n",
        "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "\n",
        "# Define the search parameters\n",
        "def pubmed_search(query, batch_size=100):\n",
        "    all_pmids = []\n",
        "    start = 0\n",
        "\n",
        "    while True:\n",
        "        search_params = {\n",
        "            \"db\": \"pubmed\",              # PubMed database\n",
        "            \"term\": query,               # Search query\n",
        "            \"retmax\": batch_size,        # Number of results per request\n",
        "            \"retstart\": start,           # Starting point for each batch\n",
        "            \"retmode\": \"xml\"             # Return results in XML format\n",
        "        }\n",
        "\n",
        "        # Make the search request\n",
        "        response = requests.get(base_url, params=search_params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            # Parse the XML response to extract the PubMed IDs (PMIDs)\n",
        "            data = xmltodict.parse(response.content)\n",
        "            id_list = data.get('eSearchResult', {}).get('IdList')\n",
        "\n",
        "            # If IdList exists, extract Ids; otherwise, break if None or empty\n",
        "            if id_list:\n",
        "                pmids = id_list.get('Id', [])\n",
        "                if isinstance(pmids, list):\n",
        "                    all_pmids.extend(pmids)\n",
        "                else:\n",
        "                    all_pmids.append(pmids)\n",
        "            else:\n",
        "                break  # Stop the loop if no more IDs are found\n",
        "\n",
        "            # Increment the start for the next batch\n",
        "            start += batch_size\n",
        "\n",
        "            # Add a delay to avoid overloading the server\n",
        "            time.sleep(0.4)\n",
        "        else:\n",
        "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "    return all_pmids\n",
        "\n",
        "# Function to fetch paper details using PubMed IDs\n",
        "# Function to fetch paper details using PubMed IDs\n",
        "def fetch_paper_details(pmids):\n",
        "    articles = []\n",
        "\n",
        "    for pmid in pmids:\n",
        "        fetch_params = {\n",
        "            \"db\": \"pubmed\",\n",
        "            \"id\": pmid,            # PubMed ID\n",
        "            \"retmode\": \"xml\"\n",
        "        }\n",
        "\n",
        "        # Make the fetch request\n",
        "        response = requests.get(fetch_url, params=fetch_params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = xmltodict.parse(response.content)\n",
        "            # Check if 'PubmedArticleSet' and 'PubmedArticle' are present\n",
        "            if 'PubmedArticleSet' in data and 'PubmedArticle' in data['PubmedArticleSet']:\n",
        "                article = data['PubmedArticleSet']['PubmedArticle']['MedlineCitation']['Article']\n",
        "\n",
        "                # Extract the relevant information\n",
        "                title = article.get('ArticleTitle', 'N/A')\n",
        "                abstract = article.get('Abstract', {}).get('AbstractText', 'N/A')\n",
        "                journal = article.get('Journal', {}).get('Title', 'N/A')\n",
        "                year = article.get('Journal', {}).get('JournalIssue', {}).get('PubDate', {}).get('Year', 'N/A')\n",
        "                authors_list = article.get('AuthorList', {}).get('Author', [])\n",
        "\n",
        "                # Handle cases where authors_list may not be a list\n",
        "                if isinstance(authors_list, list):\n",
        "                    authors = ', '.join([f\"{a['LastName']} {a.get('ForeName', '')}\" for a in authors_list if 'LastName' in a])\n",
        "                else:\n",
        "                    authors = f\"{authors_list.get('LastName', 'N/A')} {authors_list.get('ForeName', '')}\"\n",
        "\n",
        "                # Handle abstract parsing (if it's a list of dictionaries)\n",
        "                if isinstance(abstract, list):\n",
        "                    abstract_text = ' '.join([a['#text'] if '#text' in a else str(a) for a in abstract])\n",
        "                else:\n",
        "                    abstract_text = abstract\n",
        "\n",
        "                # Store the article details\n",
        "                articles.append({\n",
        "                    \"Title\": title,\n",
        "                    \"Authors\": authors,\n",
        "                    \"Abstract\": abstract_text,\n",
        "                    \"Year\": year,\n",
        "                    \"Journal\": journal,\n",
        "                    \"PMID\": pmid,\n",
        "                    \"URL\": f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}/\"\n",
        "                })\n",
        "            else:\n",
        "                print(f\"No 'PubmedArticle' found for PMID {pmid}\")\n",
        "        else:\n",
        "            print(f\"Failed to fetch details for PMID {pmid}: {response.status_code}\")\n",
        "\n",
        "        # Add a delay to avoid rate limiting\n",
        "        time.sleep(0.4)\n",
        "\n",
        "    return articles\n",
        "\n",
        "\n",
        "# Search for papers related to Autonomous Vehicles\n",
        "query = \"Autonomous Car Ethics\"\n",
        "pmids = pubmed_search(query)  # Fetch all PMIDs related to the query\n",
        "\n",
        "# Fetch the details of the retrieved papers\n",
        "papers = fetch_paper_details(pmids)\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "df.to_csv(\"pubmed_Autonomous_Car_Ethics_papers.csv\", index=False)\n",
        "\n",
        "print(\"Data scraped and saved to 'pubmed_Autonomous_Car_Ethics_papers.csv'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kinwc0-0MeKS",
        "outputId": "b3ed5d8f-6552-40c6-e43f-cfa3a3e1fa28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to retrieve data: 500\n",
            "Data scraped and saved to 'pubmed_Autonomous_Car_Ethics_papers.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZVrwpeOBx9D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}