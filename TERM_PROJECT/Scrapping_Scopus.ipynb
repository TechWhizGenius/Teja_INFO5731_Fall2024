{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechWhizGenius/Teja_INFO5731_Fall2024/blob/main/TERM_PROJECT/Scrapping_Scopus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ8L819Ic6fJ"
      },
      "source": [
        "This file contains scraping data from scopus. Its observed that the used api couldn't extract some abstracts. And so, we are removing all those papers once collected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSsp1uBpdG07"
      },
      "outputs": [],
      "source": [
        "# Importing necessary packages\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BioDKC0dMCX",
        "outputId": "b2fcc18a-8c5a-45df-916a-cf7d7c2b7ace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping papers for query: \"Autonomous Navigation\"\n",
            "Total results for query '\"Autonomous Navigation\"': 31426\n",
            "How many results would you like to scrape (up to 31426)? 5000\n",
            "Scraping completed and data saved to 'scopus_papers_Artificial Intelligence in Transportation.csv'.\n",
            "                                               Title        Authors  \\\n",
            "0  BEV perception for autonomous driving: State o...        Zhao J.   \n",
            "1  USV formation navigation decision-making throu...         Cui Z.   \n",
            "2  Integrated model of cerebellal supervised lear...          Wu Z.   \n",
            "3  Robotic destructive and nondestructive testing...  Roudsari S.S.   \n",
            "4  Ground-based on-line weed control using comput...   Abouzahir S.   \n",
            "\n",
            "                                            Abstract  Year  \\\n",
            "0  The remarkable performance of Birdâ€™s Eye View ...  2024   \n",
            "1  To address the challenging of balancing Unmann...  2024   \n",
            "2  Behavioral decision-making in unknown environm...  2024   \n",
            "3  Structural elements may develop defects due to...  2024   \n",
            "4  The detrimental effects of weeds on crop growt...  2024   \n",
            "\n",
            "                         Journal/Conference                           DOI  \\\n",
            "0          Expert Systems with Applications    10.1016/j.eswa.2024.125103   \n",
            "1          Expert Systems with Applications    10.1016/j.eswa.2024.124906   \n",
            "2                Cognitive Systems Research  10.1016/j.cogsys.2024.101302   \n",
            "3                                Structures  10.1016/j.istruc.2024.107564   \n",
            "4  Computers and Electronics in Agriculture  10.1016/j.compag.2024.109577   \n",
            "\n",
            "                                                 URL  \n",
            "0  https://api.elsevier.com/content/abstract/scop...  \n",
            "1  https://api.elsevier.com/content/abstract/scop...  \n",
            "2  https://api.elsevier.com/content/abstract/scop...  \n",
            "3  https://api.elsevier.com/content/abstract/scop...  \n",
            "4  https://api.elsevier.com/content/abstract/scop...  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# API keys and URLs\n",
        "SCOPUS_API_KEY = 'fc8e266f77097a834ac8a98322346ea4'  # Scopus API Key\n",
        "ARTICLE_API_KEY = 'fc8e266f77097a834ac8a98322346ea4'  # Article retrieval API Key\n",
        "\n",
        "search_url = \"https://api.elsevier.com/content/search/scopus\"\n",
        "article_url = \"https://api.elsevier.com/content/article/doi/\"\n",
        "\n",
        "scopus_headers = {\n",
        "    'X-ELS-APIKey': SCOPUS_API_KEY,\n",
        "    'Accept': 'application/json'\n",
        "}\n",
        "\n",
        "article_headers = {\n",
        "    'X-ELS-APIKey': ARTICLE_API_KEY,\n",
        "    'Accept': 'text/xml'\n",
        "}\n",
        "\n",
        "# Define keywords to search\n",
        "keywords = [\n",
        "    '\"Autonomous Navigation\"'\n",
        "]\n",
        "\n",
        "start_year = 2010\n",
        "end_year = 2024\n",
        "\n",
        "# Function to scrape search results from Scopus API\n",
        "def scopus_search(query, max_results=None, check_total_only=False):\n",
        "    papers = []\n",
        "    count_per_page = 25  # Results per page\n",
        "    total_results = 0\n",
        "\n",
        "    # Initial call to get total results\n",
        "    if check_total_only:\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'count': 1,\n",
        "            'start': 0,\n",
        "            'date': f'{start_year}-{end_year}',\n",
        "            'sort': 'relevance'\n",
        "        }\n",
        "        response = requests.get(search_url, headers=scopus_headers, params=params)\n",
        "        data = response.json()\n",
        "        total_results = int(data['search-results']['opensearch:totalResults'])\n",
        "        return total_results  # Return only the total count if check_total_only is True\n",
        "\n",
        "    # Loop through pages to collect results\n",
        "    for start in range(0, max_results or total_results, count_per_page):\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'count': count_per_page,\n",
        "            'start': start,\n",
        "            'date': f'{start_year}-{end_year}',\n",
        "            'sort': 'relevance'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(search_url, headers=scopus_headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            entries = data.get('search-results', {}).get('entry', [])\n",
        "            total_results += len(entries)\n",
        "\n",
        "            # Process each paper entry\n",
        "            for entry in entries:\n",
        "                title = entry.get('dc:title', 'N/A')\n",
        "                authors = entry.get('dc:creator', 'N/A')\n",
        "                year = entry.get('prism:coverDate', 'N/A').split('-')[0]\n",
        "                journal = entry.get('prism:publicationName', 'N/A')\n",
        "                doi = entry.get('prism:doi', 'N/A')\n",
        "                url = entry.get('link')[0].get('@href', 'N/A') if entry.get('link') else 'N/A'\n",
        "\n",
        "                # Retrieve the abstract using the DOI if available\n",
        "                abstract = get_abstract(doi) if doi != 'N/A' else 'N/A'\n",
        "\n",
        "                papers.append({\n",
        "                    'Title': title,\n",
        "                    'Authors': authors,\n",
        "                    'Abstract': abstract,\n",
        "                    'Year': year,\n",
        "                    'Journal/Conference': journal,\n",
        "                    'DOI': doi,\n",
        "                    'URL': url\n",
        "                })\n",
        "\n",
        "            if len(papers) >= (max_results or total_results):\n",
        "                break\n",
        "\n",
        "            time.sleep(1)  # API rate limit delay\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred while fetching data: {e}\")\n",
        "            break\n",
        "\n",
        "    return papers[:max_results]\n",
        "\n",
        "# Function to retrieve abstract using the Article API\n",
        "def get_abstract(doi):\n",
        "    try:\n",
        "        response = requests.get(f\"{article_url}{doi}\", headers=article_headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.text\n",
        "            # Extract abstract content from the XML\n",
        "            start = data.find(\"<dc:description>\") + len(\"<dc:description>\")\n",
        "            end = data.find(\"</dc:description>\")\n",
        "            abstract = data[start:end].strip() if start != -1 and end != -1 else \"N/A\"\n",
        "            return abstract\n",
        "        else:\n",
        "            #print(f\"Error: {response.status_code} for DOI {doi}\")\n",
        "            return \"N/A\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching abstract for DOI {doi}: {e}\")\n",
        "        return \"N/A\"\n",
        "\n",
        "# Main function to run the scraper\n",
        "def main():\n",
        "    all_papers = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"Scraping papers for query: {keyword}\")\n",
        "\n",
        "        # Get the total number of results for the query\n",
        "        total_results = scopus_search(keyword, check_total_only=True)\n",
        "        print(f\"Total results for query '{keyword}': {total_results}\")\n",
        "\n",
        "        # Ask the user for the number of results they want\n",
        "        max_results = int(input(f\"How many results would you like to scrape (up to {total_results})? \"))\n",
        "        max_results = min(max_results, total_results)  # Ensure it doesn't exceed total results\n",
        "\n",
        "        papers = scopus_search(keyword, max_results=max_results)\n",
        "        all_papers.extend(papers)\n",
        "\n",
        "    # Convert the list of papers to a DataFrame and save it to CSV\n",
        "    df = pd.DataFrame(all_papers)\n",
        "    df.to_csv('scopus_papers_Artificial Intelligence in Transportation.csv', index=False)\n",
        "    print(\"Scraping completed and data saved to 'scopus_papers_Artificial Intelligence in Transportation.csv'.\")\n",
        "    print(df.head())\n",
        "\n",
        "# Run the scraper\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkAl724EdZiO",
        "outputId": "03ed6abf-8a3d-479a-9a3a-c50cb0cfd932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               Title       Authors  \\\n",
            "0  Nighttime fog and low stratus detection under ...      Jiang J.   \n",
            "1  A machine learning comparison of transportatio...    Banyong C.   \n",
            "2  The impact of smart city construction on achie...       Yang X.   \n",
            "3      GLAN: A graph-based linear assignment network        Liu H.   \n",
            "4  Obvious artificial intelligence-generated anom...  Gulumbe B.H.   \n",
            "\n",
            "                                            Abstract  Year  \\\n",
            "0  A scheme for satellite remote sensing is propo...  2024   \n",
            "1  Thailand's collaboration with China to develop...  2024   \n",
            "2  Amidst the intensification of global warming c...  2024   \n",
            "3  Differentiable solvers for the linear assignme...  2024   \n",
            "4                                                NaN  2024   \n",
            "\n",
            "                                  Journal/Conference  \\\n",
            "0  ISPRS Journal of Photogrammetry and Remote Sen...   \n",
            "1                             Results in Engineering   \n",
            "2                                    Land Use Policy   \n",
            "3                                Pattern Recognition   \n",
            "4                                 Learned Publishing   \n",
            "\n",
            "                                DOI  \\\n",
            "0   10.1016/j.isprsjprs.2024.10.014   \n",
            "1      10.1016/j.rineng.2024.103110   \n",
            "2  10.1016/j.landusepol.2024.107372   \n",
            "3      10.1016/j.patcog.2024.110694   \n",
            "4                 10.1002/leap.1626   \n",
            "\n",
            "                                                 URL  \n",
            "0  https://api.elsevier.com/content/abstract/scop...  \n",
            "1  https://api.elsevier.com/content/abstract/scop...  \n",
            "2  https://api.elsevier.com/content/abstract/scop...  \n",
            "3  https://api.elsevier.com/content/abstract/scop...  \n",
            "4  https://api.elsevier.com/content/abstract/scop...  \n",
            "----------------------------------------------------------------------------------------------------------\n",
            "The Size of the Dataframe is  (344, 7)\n",
            "Number of null values in abstrct column is  285\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('scopus_papers_Artificial Intelligence in Transportation.csv')\n",
        "print(df.head())\n",
        "\n",
        "print('----------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "print('The Size of the Dataframe is ',df.shape)\n",
        "print('Number of null values in abstrct column is ', df['Abstract'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YJR3BojOeN_a",
        "outputId": "eedbeb91-de34-4699-cc51-84e67a35e9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of rows in final scraped dataframe is  (59, 7)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_69eabe15-7b80-4458-a8f9-a71bd8332fec\", \"scopus_papers_Artificial Intelligence in Transportation.csv\", 95738)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Remove rows with null values in the 'Abstract' column\n",
        "df_cleaned = df.dropna(subset=['Abstract'])\n",
        "\n",
        "# Save the cleaned DataFrame to a CSV file\n",
        "df_cleaned.to_csv('scopus_papers_Artificial Intelligence in Transportation.csv', index=False)\n",
        "\n",
        "print('The number of rows in final scraped dataframe is ', df_cleaned.shape)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download('scopus_papers_Artificial Intelligence in Transportation.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aImjuSrYgMIP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Cos44d1jUYj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}