{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TechWhizGenius/Teja_INFO5731_Fall2024/blob/main/Mandaloju_Teja_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00da596-56a3-444d-e900-dd5d0f3158d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,027 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,447 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,596 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,590 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,362 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Fetched 16.4 MB in 7s (2,254 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Installing necessary packages\n",
        "!apt-get update\n",
        "!apt-get install -y wget unzip\n",
        "!pip install selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Google Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y\n",
        "\n",
        "# Check Chrome version\n",
        "!google-chrome --version\n",
        "\n",
        "# Downloading the correct version of ChromeDriver (matching Chrome 128.0.6613.137)\n",
        "!wget https://storage.googleapis.com/chrome-for-testing-public/128.0.6613.137/linux64/chromedriver-linux64.zip\n",
        "\n",
        "# Unzip ChromeDriver directly to /usr/local/bin\n",
        "!unzip chromedriver-linux64.zip -d /usr/local/bin/\n",
        "\n",
        "# Move the extracted chromedriver to the correct location and set permissions\n",
        "!sudo mv /usr/local/bin/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver\n",
        "!sudo chmod +x /usr/local/bin/chromedriver\n",
        "\n",
        "# Verify ChromeDriver installation\n",
        "!chromedriver --version\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzHY8XXf0-_r",
        "outputId": "23b15bd7-c30b-4909-e9ec-26f119cc8db7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-03 00:34:49--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 172.217.214.93, 172.217.214.136, 172.217.214.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|172.217.214.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 111878140 (107M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 106.69M  55.0MB/s    in 1.9s    \n",
            "\n",
            "2024-10-03 00:34:51 (55.0 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [111878140/111878140]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 123620 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (129.0.6668.89-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libvulkan1; however:\n",
            "  Package libvulkan1 is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.2 [10.7 MB]\n",
            "Fetched 10.9 MB in 1s (12.7 MB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 123737 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up google-chrome-stable (129.0.6668.89-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Google Chrome 129.0.6668.89 \n",
            "--2024-10-03 00:35:25--  https://storage.googleapis.com/chrome-for-testing-public/128.0.6613.137/linux64/chromedriver-linux64.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.206.207, 108.177.121.207, 209.85.145.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.206.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9370775 (8.9M) [application/zip]\n",
            "Saving to: ‘chromedriver-linux64.zip’\n",
            "\n",
            "chromedriver-linux6 100%[===================>]   8.94M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-10-03 00:35:25 (223 MB/s) - ‘chromedriver-linux64.zip’ saved [9370775/9370775]\n",
            "\n",
            "Archive:  chromedriver-linux64.zip\n",
            "  inflating: /usr/local/bin/chromedriver-linux64/LICENSE.chromedriver  \n",
            "  inflating: /usr/local/bin/chromedriver-linux64/THIRD_PARTY_NOTICES.chromedriver  \n",
            "  inflating: /usr/local/bin/chromedriver-linux64/chromedriver  \n",
            "ChromeDriver 128.0.6613.137 (fe621c5aa2d6b987e964fb1b5066833da5fb613d-refs/branch-heads/6613@{#1711})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up Chrome options for headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Ensure GUI is off\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Set up the ChromeDriver service (use your correct path)\n",
        "service = Service('/usr/local/bin/chromedriver')\n",
        "\n",
        "# Initialize WebDriver\n",
        "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
        "\n",
        "# Define the function to fetch IMDb reviews\n",
        "def fetch_imdb_reviews(movie_url, num_reviews):\n",
        "    driver.get(movie_url)\n",
        "\n",
        "    reviews = []\n",
        "\n",
        "    while len(reviews) < num_reviews:\n",
        "        try:\n",
        "            # Wait for review elements to be present\n",
        "            review_elements = WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.lister-item'))\n",
        "            )\n",
        "\n",
        "            for review_element in review_elements:\n",
        "                if len(reviews) >= num_reviews:\n",
        "                    break\n",
        "                try:\n",
        "                    # Extract review title\n",
        "                    review_title = review_element.find_element(By.CSS_SELECTOR, '.title').text.strip()\n",
        "                    # Extract review text\n",
        "                    review_text = review_element.find_element(By.CSS_SELECTOR, '.text.show-more__control').text.strip()\n",
        "                    # Extract rating (if available)\n",
        "                    try:\n",
        "                        rating_element = review_element.find_element(By.CSS_SELECTOR, '.rating-other-user-rating span')\n",
        "                        rating = rating_element.text.strip().split(\"/\")[0]  # Get the numeric part before '/'\n",
        "                    except NoSuchElementException:\n",
        "                        rating = \"No Rating\"\n",
        "\n",
        "                    reviews.append({\n",
        "                        'Review Title': review_title,\n",
        "                        'Review Text': review_text,\n",
        "                        'Rating': rating\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting review info: {e}\")\n",
        "\n",
        "            try:\n",
        "                # Click the 'Load More' button to load more reviews\n",
        "                load_more_button = driver.find_element(By.CSS_SELECTOR, '.load-more-data')\n",
        "                load_more_button.click()\n",
        "                time.sleep(2)  # Allow time for the new reviews to load\n",
        "            except NoSuchElementException:\n",
        "                print(\"No more reviews or 'Load More' button not found.\")\n",
        "                break\n",
        "\n",
        "        except TimeoutException:\n",
        "            print(\"Timed out waiting for reviews.\")\n",
        "            break\n",
        "\n",
        "    return reviews\n",
        "\n",
        "# Fetch and save the data\n",
        "movie_url = 'https://www.imdb.com/title/tt15398776/reviews'  # Example movie URL (Oppenheimer)\n",
        "num_reviews = 1000  # Number of reviews to scrape\n",
        "reviews = fetch_imdb_reviews(movie_url, num_reviews)\n",
        "\n",
        "# Convert to DataFrame and save to CSV\n",
        "df = pd.DataFrame(reviews)\n",
        "df.to_csv('imdb_reviews.csv', index=False)\n",
        "\n",
        "# Quit the WebDriver\n",
        "driver.quit()\n",
        "\n",
        "print(f\"Scraped {len(reviews)} reviews and saved to 'imdb_reviews.csv'\")\n",
        "print(df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReHsOT2g7Gwg",
        "outputId": "8cbaf377-4976-4467-8e00-668facf63a68"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped 1000 reviews and saved to 'imdb_reviews.csv'\n",
            "                                        Review Title  \\\n",
            "0                              Murphy is exceptional   \n",
            "1  A challenging watch to be sure, but a worthwhi...   \n",
            "2                             Quality but exhausting   \n",
            "3                           And the Oscar goes to...   \n",
            "4  A brilliantly layered examination of a man thr...   \n",
            "5                                    Is it just me ?   \n",
            "6      Nolan touches greatness, falls slightly short   \n",
            "7                                 Severely overhyped   \n",
            "8       A Cinematic Masterpiece by Christopher Nolan   \n",
            "9                                      A Masterpiece   \n",
            "\n",
            "                                         Review Text     Rating  \n",
            "0  You'll have to have your wits about you and yo...          9  \n",
            "1                                                             8  \n",
            "2  I'm a big fan of Nolan's work so was really lo...          7  \n",
            "3  I'm still collecting my thoughts after experie...         10  \n",
            "4  \"Oppenheimer\" is a biographical thriller film ...         10  \n",
            "5  Is it just me or did anyone else find this mov...          8  \n",
            "6                                                             8  \n",
            "7  I align with other reviewers here who have fou...  No Rating  \n",
            "8  After a busy career filled with masterpieces, ...         10  \n",
            "9  I may consider myself lucky to be alive to wat...         10  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5a553e-6766-4580-d686-07d9534c4658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         Review Text  \\\n",
            "0  You'll have to have your wits about you and yo...   \n",
            "1                                                      \n",
            "2  I'm a big fan of Nolan's work so was really lo...   \n",
            "3  I'm still collecting my thoughts after experie...   \n",
            "4  \"Oppenheimer\" is a biographical thriller film ...   \n",
            "5  Is it just me or did anyone else find this mov...   \n",
            "6                                                      \n",
            "7  I align with other reviewers here who have fou...   \n",
            "8  After a busy career filled with masterpieces, ...   \n",
            "9  I may consider myself lucky to be alive to wat...   \n",
            "\n",
            "                                 Cleaned Review Text  \n",
            "0  youll wit brain fulli switch watch oppenheim c...  \n",
            "1                                                     \n",
            "2  im big fan nolan work realli look forward unde...  \n",
            "3  im still collect thought experienc film cillia...  \n",
            "4  oppenheim biograph thriller film written direc...  \n",
            "5  anyon el find movi hate say bore know know pie...  \n",
            "6                                                     \n",
            "7  align review found despit hype surround oppenh...  \n",
            "8  busi career fill masterpiec christoph nolan fi...  \n",
            "9  may consid lucki aliv watch christoph nolan wo...  \n",
            "                                        Review Title  \\\n",
            "0                              Murphy is exceptional   \n",
            "1  A challenging watch to be sure, but a worthwhi...   \n",
            "2                             Quality but exhausting   \n",
            "3                           And the Oscar goes to...   \n",
            "4  A brilliantly layered examination of a man thr...   \n",
            "5                                    Is it just me ?   \n",
            "6      Nolan touches greatness, falls slightly short   \n",
            "7                                 Severely overhyped   \n",
            "8       A Cinematic Masterpiece by Christopher Nolan   \n",
            "9                                      A Masterpiece   \n",
            "\n",
            "                                Cleaned Review Title  \n",
            "0                                      murphi except  \n",
            "1                  challeng watch sure worthwhil one  \n",
            "2                                    qualiti exhaust  \n",
            "3                                          oscar goe  \n",
            "4  brilliantli layer examin man throughout incred...  \n",
            "5                                                     \n",
            "6              nolan touch great fall slightli short  \n",
            "7                                      sever overhyp  \n",
            "8                 cinemat masterpiec christoph nolan  \n",
            "9                                         masterpiec  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#df = pd.read_csv('imdb_reviews.csv')\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string for non-string inputs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)     # Remove noise\n",
        "    text = re.sub(r'\\d+', '', text)    # Remove numbers\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word.lower() not in stop_words)     # Remove stopwords\n",
        "    text = text.lower()                                                       # lowercasing all texts\n",
        "    text = ' '.join(stemmer.stem(word) for word in text.split())     # Stemming\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())    # Lemmatization\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'Review Text' column\n",
        "df['Cleaned Review Text'] = df['Review Text'].apply(clean_text)\n",
        "df['Cleaned Review Title'] = df['Review Title'].apply(clean_text)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('imdb_reviews_cleaned.csv', index=False)\n",
        "\n",
        "df.to_csv('imdb_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(df[['Review Text', 'Cleaned Review Text']].head(10))\n",
        "print(df[['Review Title', 'Cleaned Review Title']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5596e9-884e-40fc-bdcc-7b7e7f1f4a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "#df = pd.read_csv('imdb_reviews_cleaned.csv')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm') # Load the spaCy model\n",
        "\n",
        "# Part (1): POS Tagging and Count\n",
        "def pos_tagging_and_count(text):\n",
        "    if isinstance(text, str):  # Check if the input is a string\n",
        "        tokens = word_tokenize(text)\n",
        "        tagged = pos_tag(tokens)\n",
        "        counts = {\n",
        "            'Noun': sum(1 for word, tag in tagged if tag.startswith('NN')),\n",
        "            'Verb': sum(1 for word, tag in tagged if tag.startswith('VB')),\n",
        "            'Adjective': sum(1 for word, tag in tagged if tag.startswith('JJ')),\n",
        "            'Adverb': sum(1 for word, tag in tagged if tag.startswith('RB')),\n",
        "        }\n",
        "        return counts\n",
        "    else:\n",
        "        return {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}  # Return zero counts for non-strings\n",
        "\n",
        "# Calculate POS counts for each review\n",
        "df['POS Count Text'] = df['Cleaned Review Text'].apply(pos_tagging_and_count)\n",
        "df['POS Count Title'] = df['Cleaned Review Title'].apply(pos_tagging_and_count)\n",
        "\n",
        "# Combine counts into a single DataFrame\n",
        "pos_summary01 = pd.DataFrame(df['POS Count Text'].tolist()).sum().reset_index()\n",
        "pos_summary02 =pd.DataFrame(df['POS Count Title'].tolist()).sum().reset_index()\n",
        "pos_summary01.columns = ['Part of Speech', 'Count']\n",
        "pos_summary02.columns = ['Part of Speech', 'Count']\n",
        "print(\"Parts of Speech Counts:\")\n",
        "print(pos_summary01)\n",
        "print(pos_summary02)\n",
        "\n",
        "# Part (2): Parsing Trees\n",
        "def parsing_trees(text):\n",
        "    if isinstance(text, str):  # Check if the input is a string\n",
        "        doc = nlp(text)\n",
        "        # Print the constituency parsing tree\n",
        "        print(\"\\nConstituency Parsing Tree:\")\n",
        "        for sent in doc.sents:\n",
        "            print(sent.root)  # Printing the root node for simplicity\n",
        "\n",
        "        # Dependency Parsing\n",
        "        dependency_tree = [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "        return dependency_tree\n",
        "    else:\n",
        "        return []  # Return empty for non-strings\n",
        "\n",
        "# Example for parsing\n",
        "sample_text01 = df['Cleaned Review Text'].iloc[0]\n",
        "dependency_tree01 = parsing_trees(sample_text01)\n",
        "\n",
        "sample_text02 = df['Cleaned Review Title'].iloc[0]\n",
        "dependency_tree02 = parsing_trees(sample_text02)\n",
        "\n",
        "print(\"\\nDependency Parsing Tree:\")\n",
        "print(dependency_tree01)  # Printing dependency relations\n",
        "print(dependency_tree02)  # Printing dependency relations\n",
        "\n",
        "# Part (3): Named Entity Recognition (NER)\n",
        "def extract_entities(text):\n",
        "    if isinstance(text, str):  # Check if the input is a string\n",
        "        doc = nlp(text)\n",
        "        entities = {}\n",
        "        for ent in doc.ents:\n",
        "            entities[ent.label_] = entities.get(ent.label_, 0) + 1\n",
        "        return entities\n",
        "    else:\n",
        "        return {}  # Return empty dictionary for non-strings\n",
        "\n",
        "# Extract entities for each review\n",
        "df['Entities01'] = df['Cleaned Review Text'].apply(extract_entities)\n",
        "df['Entities02'] = df['Cleaned Review Title'].apply(extract_entities)\n",
        "\n",
        "# Summarizing the counts of entities\n",
        "entity_summary01 = {}\n",
        "for entity_counts in df['Entities01']:\n",
        "    for entity, count in entity_counts.items():\n",
        "        entity_summary01[entity] = entity_summary01.get(entity, 0) + count\n",
        "\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "print(entity_summary01)\n",
        "\n",
        "entity_summary02 = {}\n",
        "for entity_counts in df['Entities02']:\n",
        "    for entity, count in entity_counts.items():\n",
        "        entity_summary02[entity] = entity_summary02.get(entity, 0) + count\n",
        "\n",
        "print(\"\\nNamed Entity Counts:\")\n",
        "print(entity_summary02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPcet_jDrTt",
        "outputId": "efec8559-e5ab-4501-9ec3-be8d90b2cd05"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parts of Speech Counts:\n",
            "  Part of Speech  Count\n",
            "0           Noun  55112\n",
            "1           Verb  11739\n",
            "2      Adjective  20748\n",
            "3         Adverb   4557\n",
            "  Part of Speech  Count\n",
            "0           Noun   2254\n",
            "1           Verb    229\n",
            "2      Adjective    693\n",
            "3         Adverb    126\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "absolut\n",
            "say\n",
            "\n",
            "Constituency Parsing Tree:\n",
            "murphi\n",
            "\n",
            "Dependency Parsing Tree:\n",
            "[('you', 'nsubj', 'wit'), ('ll', 'aux', 'wit'), ('wit', 'nsubj', 'absolut'), ('brain', 'compound', 'switch'), ('fulli', 'compound', 'switch'), ('switch', 'compound', 'watch'), ('watch', 'dobj', 'wit'), ('oppenheim', 'nsubj', 'easili'), ('could', 'aux', 'easili'), ('easili', 'ccomp', 'wit'), ('get', 'conj', 'wit'), ('away', 'prt', 'get'), ('nonattent', 'compound', 'filmmak'), ('viewer', 'compound', 'intellig'), ('intellig', 'compound', 'filmmak'), ('filmmak', 'nsubj', 'show'), ('show', 'conj', 'wit'), ('audienc', 'nmod', 'fire'), ('great', 'amod', 'respect'), ('respect', 'compound', 'fire'), ('fire', 'nmod', 'inform'), ('dialogu', 'amod', 'pack'), ('pack', 'nsubj', 'inform'), ('inform', 'conj', 'wit'), ('relentless', 'amod', 'jump'), ('pace', 'compound', 'jump'), ('jump', 'dobj', 'inform'), ('differ', 'conj', 'wit'), ('time', 'dobj', 'differ'), ('oppenheim', 'compound', 'life'), ('life', 'compound', 'continu'), ('continu', 'nmod', 'clue'), ('hour', 'nmod', 'clue'), ('runtim', 'nmod', 'clue'), ('visual', 'amod', 'clue'), ('clue', 'compound', 'guid'), ('guid', 'appos', 'time'), ('viewer', 'amod', 'time'), ('time', 'dobj', 'wit'), ('you', 'nsubj', 'get'), ('ll', 'aux', 'get'), ('get', 'relcl', 'time'), ('grip', 'nsubj', 'quit'), ('quit', 'ccomp', 'wit'), ('quickli', 'dobj', 'quit'), ('relentless', 'amod', 'help'), ('help', 'aux', 'express'), ('express', 'conj', 'wit'), ('urgenc', 'compound', 'germani'), ('u', 'compound', 'chase'), ('attack', 'compound', 'chase'), ('chase', 'compound', 'germani'), ('atom', 'compound', 'bomb'), ('bomb', 'compound', 'germani'), ('germani', 'dobj', 'express'), ('could', 'aux', 'absolut'), ('absolut', 'ROOT', 'absolut'), ('career', 'nmod', 'oscar'), ('best', 'advmod', 'career'), ('perform', 'compound', 'brilliant'), ('consistenli', 'compound', 'brilliant'), ('brilliant', 'compound', 'oscar'), ('cillian', 'compound', 'murphi'), ('murphi', 'compound', 'oscar'), ('anchor', 'compound', 'oscar'), ('film', 'compound', 'oscar'), ('nail', 'compound', 'oscar'), ('oscar', 'nsubj', 'perform'), ('perform', 'ccomp', 'absolut'), ('fact', 'dobj', 'perform'), ('whole', 'amod', 'fantast'), ('cast', 'compound', 'fantast'), ('fantast', 'npadvmod', 'perform'), ('apart', 'prep', 'perform'), ('mayb', 'compound', 'emili'), ('sometim', 'nmod', 'emili'), ('overwrought', 'compound', 'emili'), ('emili', 'compound', 'perform'), ('blunt', 'compound', 'perform'), ('perform', 'compound', 'rdj'), ('rdj', 'conj', 'perform'), ('also', 'advmod', 'perform'), ('particularli', 'amod', 'return'), ('brilliant', 'amod', 'return'), ('return', 'dobj', 'perform'), ('proper', 'amod', 'act'), ('act', 'compound', 'layer'), ('decad', 'compound', 'call'), ('call', 'compound', 'layer'), ('screenplay', 'compound', 'layer'), ('den', 'compound', 'layer'), ('layer', 'appos', 'return'), ('i', 'nsubj', 'say'), ('d', 'nsubj', 'say'), ('say', 'ROOT', 'say'), ('thick', 'amod', 'cinematographi'), ('bibl', 'compound', 'cinematographi'), ('cinematographi', 'nsubj', 'quit'), ('quit', 'ccomp', 'say'), ('stark', 'amod', 'moment'), ('spare', 'amod', 'part'), ('part', 'nmod', 'imbu'), ('imbu', 'nmod', 'luciou'), ('rich', 'amod', 'luciou'), ('luciou', 'compound', 'moment'), ('colour', 'compound', 'moment'), ('moment', 'dobj', 'quit'), ('especi', 'ccomp', 'quit'), ('scene', 'compound', 'pugh'), ('florenc', 'compound', 'pugh'), ('pugh', 'compound', 'beauti'), ('score', 'compound', 'beauti'), ('beauti', 'compound', 'time'), ('time', 'npadvmod', 'especi'), ('mostli', 'nsubj', 'anxiou'), ('anxiou', 'relcl', 'time'), ('oppress', 'compound', 'ad'), ('ad', 'nmod', 'hour'), ('relentless', 'amod', 'pace'), ('pace', 'compound', 'hour'), ('hour', 'dobj', 'anxiou'), ('runtim', 'amod', 'fli'), ('fli', 'nsubj', 'found'), ('found', 'conj', 'say'), ('intens', 'dobj', 'found'), ('tax', 'compound', 'highli'), ('highli', 'compound', 'reward'), ('reward', 'compound', 'film'), ('watch', 'compound', 'film'), ('film', 'nsubj', 'make'), ('make', 'ccomp', 'found'), ('finest', 'amod', 'watch'), ('realli', 'nmod', 'watch'), ('great', 'amod', 'watch'), ('watch', 'dobj', 'make')]\n",
            "[('murphi', 'ROOT', 'murphi'), ('except', 'prep', 'murphi')]\n",
            "\n",
            "Named Entity Counts:\n",
            "{'PERSON': 2806, 'ORG': 1099, 'NORP': 722, 'EVENT': 51, 'WORK_OF_ART': 27, 'GPE': 815, 'ORDINAL': 357, 'FAC': 77, 'LOC': 25, 'DATE': 274, 'CARDINAL': 662, 'TIME': 256, 'PRODUCT': 86, 'QUANTITY': 7, 'MONEY': 4, 'LANGUAGE': 3}\n",
            "\n",
            "Named Entity Counts:\n",
            "{'PERSON': 121, 'GPE': 39, 'ORG': 71, 'NORP': 29, 'TIME': 24, 'CARDINAL': 7, 'ORDINAL': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('imdb_reviews_cleaned.csv', index=False)\n",
        "\n",
        "print(\"Cleaned data saved to 'imdb_reviews_cleaned.csv'\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('imdb_reviews_cleaned.csv')"
      ],
      "metadata": {
        "id": "qYRO5Cn8bYwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07258ce2-dc76-4f4a-cb4e-6b97ec20d249"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to 'imdb_reviews_cleaned.csv'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8538533e-48df-4168-b574-b11670fb3f79\", \"imdb_reviews_cleaned.csv\", 1932297)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#link to file: https://drive.google.com/file/d/1OGv-1dnitseHaLL4ZRriXApxNyvRPjNN/view?usp=sharing"
      ],
      "metadata": {
        "id": "jb1AKmq57oNw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "\n",
        "'''\n",
        "This assignment is really good. At first, I have used selenium to scrape data from IMDB. Later, performed some data cleaning on the dataset.\n",
        "Finally, Implemented POS tagging. I have enjoyed doing all the tasks. I have spent 3 hours to complete this assignment. It is really worth it\n",
        "as it has deepened my knowledge on data cleaning techniques of a corpus.\n",
        "'''\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c6670dec-e000-4b84-a67a-e5a73ff53e97"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis assignment is really good. At first, I have used selenium to scrape data from IMDB. Later, performed some data cleaning on the dataset. \\nFinally, Implemented POS tagging. I have enjoyed doing all the tasks. I have spent 3 hours to complete this assignment. It is really worth it \\nas it has deepened my knowledge on data cleaning techniques of a corpus.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}